# ğŸ“Š Daily Sales Data Pipeline with Airflow, Snowflake, and dbt

An ETL/ELT pipeline for **retail supply chain analytics**, built with:

- **Apache Airflow** â†’ workflow orchestrator  
- **Snowflake** â†’ main data warehouse  
- **dbt (data build tool)** â†’ transformation, snapshotting, testing  
- **MySQL** â†’ operational database as the data source  

---

## ğŸš€ Key Features
- Automated schema and table creation in Snowflake  
- Extract data from MySQL (dimension & fact tables)  
- Load data into the *landing* schema in Snowflake using **Pandas + SQLAlchemy**  
- Modular pipeline (`extract.py`, `load.py`, `connection.py`, etc.) â†’ easy to maintain  
- Configurable via **Airflow Variables** (`secret_file`, `sql_file`, etc.)  
- **dbt tasks** automatically executed in a single task group:
  - `test_source` â†’ validate sources  
  - `run` â†’ run transformations  
  - `test_model` â†’ test model outputs  
  - `snapshot` â†’ perform snapshotting  

---

## ğŸ“‚ Project Structure
```bash
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ daily_sales.py 
â”‚   â”œâ”€â”€ data_dummy/
â”‚   â”‚   â””â”€â”€ store_a.sql
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”‚   â”œâ”€â”€ connection.py 
â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py 
â”‚   â”‚   â”‚   â”œâ”€â”€ load.py 
â”‚   â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”‚   â””â”€â”€ utils.py 
â”‚   â”‚   â””â”€â”€ sql/
â”‚   â”‚       â”œâ”€â”€ create_schema.sql
â”‚   â”‚       â”œâ”€â”€ dimension/
â”‚   â”‚       â”‚   â”œâ”€â”€ products.sql
â”‚   â”‚       â”‚   â”œâ”€â”€ suppliers.sql
â”‚   â”‚       â”‚   â””â”€â”€ warehouses.sql
â”‚   â”‚       â””â”€â”€ fact/
â”‚   â”‚           â”œâ”€â”€ orderitems.sql
â”‚   â”‚           â”œâ”€â”€ orders.sql
â”‚   â”‚           â”œâ”€â”€ sales.sql
â”‚   â”‚           â”œâ”€â”€ shipmentitems.sql
â”‚   â”‚           â”œâ”€â”€ shipments.sql
â”‚   â”‚           â””â”€â”€ stock.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ dags/
â”‚   â”‚       â””â”€â”€ test_dag_example.py
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â”œâ”€â”€ airflow_settings.yaml
â”‚   â”œâ”€â”€ docker-compose.override.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ packages.txt
â”‚   â””â”€â”€ requirements.txt
â”‚  
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ my_snowflake_db/
â”‚       â”œâ”€â”€ analyses/ 
â”‚       â”œâ”€â”€ macros/
â”‚       â”‚   â””â”€â”€ generate_schema_name.sql  
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ dimension/
â”‚       â”‚   â”‚   â”œâ”€â”€ dim_date.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ dim_products.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ dim_suppliers.sql
â”‚       â”‚   â”‚   â””â”€â”€ dim_warehouse.sql
â”‚       â”‚   â”œâ”€â”€ fact/
â”‚       â”‚   â”‚   â”œâ”€â”€ fact_inventory.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ fact_orders.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ fact_sales.sql
â”‚       â”‚   â”‚   â””â”€â”€ fact_shipments.sql
â”‚       â”‚   â””â”€â”€ schema_warehouse.yml
â”‚       â”œâ”€â”€ seeds/
â”‚       â”œâ”€â”€ snapshots/
â”‚       â”‚   â”œâ”€â”€ dim_product_snapshot.sql
â”‚       â”‚   â”œâ”€â”€ dim_supplier_snapshot.sql
â”‚       â”‚   â””â”€â”€ dim_warehouse_snapshot.sql
â”‚       â”œâ”€â”€ tests/
â”‚       â”‚   â”œâ”€â”€ generic/
â”‚       â”‚   â”‚   â”œâ”€â”€ test_string_and_set.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ test_validate_fact_inventory_arrays.sql
â”‚       â”‚   â”‚   â””â”€â”€ test_validate_id_date.sql
â”‚       â”‚   â””â”€â”€ singular/
â”‚       â”œâ”€â”€ .gitignore
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â”œâ”€â”€ package-lock.yml
â”‚       â”œâ”€â”€ packages.yml
â”‚       â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ docker_container.png
â”‚   â”œâ”€â”€ pipeline.png
â”‚   â”œâ”€â”€ schema_source.png
â”‚   â””â”€â”€ schema_warehouse.png
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
---

## âš™ï¸ How It Works
- **Airflow DAG** (`daily_sales.py`) orchestrates the pipeline:
    - Creates schemas/tables in Snowflake
    - Extracts data from MySQL
    - Loads raw data into Snowflake (landing schema)
    - Executes dbt tasks for transformation, testing, and snapshotting
- **Configuration** is managed via:
    - Airflow Variables (paths & secrets)
    - .secrets.toml for credentials (not committed to Git)
- **dbt Project** handles transformations and quality checks, ensuring data is production-ready.

---

## ğŸ› ï¸ Tech Stack

- Airflow `2.x`
- Snowflake
- dbt Core `1.x`
- MySQL
- Python `3.9+`
- Pandas + SQLAlchemy

---

## ğŸ“Š Pipeline, Database & BI Overview

**Pipeline Flow:**  
![Pipeline Diagram](docs/pipeline.png)

**Docker Container Architecture:**  
![Docker Architecture](docs/docker_container.png)

**Source Schema (MySQL)**
![Source Schema](docs/schema_source.png)

**Warehouse Schema (Snowflake)**
![Warehouse Schema](docs/schema_warehouse.png)

**Dashboard:**  
This shows sample visualizations based on the data loaded into Snowflake and transformed via dbt.  
![Power BI Dashboard](docs/powerbi_dashboard.png)

---

## ğŸ—„ï¸ Data Modeling

This project follows a Kimball-style dimensional modeling approach:

- **Source Schema (MySQL)**
Contains operational data from the retail system. Includes both master data and transactions:
  - `products`, `suppliers`, `warehouses`, `customers`
  - `orders`, `order_items`, `shipments`, `inventory`, `sales`

- **Warehouse Schema (Snowflake)**
Modeled into dimensions and facts for analytics:

  - Dimensions
    - `dim_date` â†’ calendar table
    - `dim_products`, `dim_suppliers`, `dim_warehouse` â†’ master entities

  - Facts
    - `fact_orders` â†’ customer order records
    - `fact_sales` â†’ sales transactions
    - `fact_shipments` â†’ logistics and delivery
    - `fact_inventory` â†’ stock levels by warehouse and product

  - Snapshots (dbt)
  Track historical changes of slowly changing dimensions (SCD Type 2):
    - `dim_product_snapshot` â†’ track product detail changes (e.g., category, price)
    - `dim_supplier_snapshot` â†’ track supplier info changes
    - `dim_warehouse_snapshot` â†’ track warehouse attributes

---

## ğŸ”‘ Portfolio Highlights

- Demonstrates end-to-end retail data pipeline
- Shows integration of Airflow, Snowflake, dbt, and Python ETL scripts
- Includes modular ETL, snapshotting, and automated data quality testing
- Fully documented and ready to showcase on GitHub

---

## âš™ï¸ Setup & Installation

Follow these steps to run the Daily Sales Data Pipeline locally using Docker and Astronomer (Astro CLI).

**1ï¸âƒ£ Install Docker**
Download and install Docker Desktop based on your operating system:

- Windows/Mac:
ğŸ‘‰ https://www.docker.com/products/docker-desktop/

- Linux (Ubuntu):
```bash
sudo apt update
sudo apt install docker.io docker-compose -y
sudo systemctl enable --now docker
```
âœ… Verify your installation:
```bash
docker ps
```

**2ï¸âƒ£ Install Astro CLI**

Astro CLI is used to manage and run Apache Airflow locally with Docker.

Install via the official script:
```bash
curl -sSL https://install.astronomer.io | bash
```
Confirm that Astro is successfully installed:
```bash
astro version
```

**3ï¸âƒ£ Pull Required Docker Images**

This project relies on the following Docker images:

  - **ghcr.io/dbt-labs/dbt-snowflake** â†’ runs dbt transformations
  - **mysql:8.0.42-debian** â†’ serves as the operational (source) database
  - **Airflow image** â†’ automatically provided by Astronomer

Pull the images manually:
```bash
docker pull ghcr.io/dbt-labs/dbt-snowflake
docker pull mysql:8.0.42-debian
```
âš ï¸ The Airflow image is automatically handled by Astronomer when running ```astro dev start```.

**4ï¸âƒ£ (Optional) Initialize an Astro Project**

If you havenâ€™t initialized an Airflow project yet, run:
```bash
astro dev init
```
This command creates the necessary project structure and default configuration files for Astronomer.

**5ï¸âƒ£ Configure Snowflake Credentials â†’** ```profiles.yml```

Update your Snowflake account credentials in:
```dbt/my_snowflake_db/profiles.yml```

Example:
```bash
my_snowflake_db:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: <your_account>
      user: <your_username>
      password: <your_password>
      role: <your_role>
      database: RETAIL_SUPPLY_CHAIN
      warehouse: COMPUTE_WH
      schema: ANALYTICS
      threads: 4
      client_session_keep_alive: False
```

ğŸ”’ Important: Do not commit this file to GitHub since it contains your Snowflake credentials.

**6ï¸âƒ£ Configure Airflow Connections â†’** ```airflow_settings.yml```

Set up your Snowflake and MySQL connections in:
```airflow/airflow_settings.yml```

Example:
```bash
connections:
  - conn_id: snowflake_conn
    conn_type: snowflake
    conn_login: <your_username>
    conn_password: <your_password>
    conn_schema: ANALYTICS
    conn_extra:
      account: <your_account>
      warehouse: COMPUTE_WH
      database: RETAIL_SUPPLY_CHAIN
      role: <your_role>

  - conn_id: mysql_source
    conn_type: mysql
    conn_host: mysql
    conn_schema: retail_supply_chain
    conn_login: root
    conn_password: root
    conn_port: 3306
```

**7ï¸âƒ£ Start the Local Environment**

Spin up the entire local stack (Airflow, MySQL, and dbt containers) with:
```bash
astro dev start
```

â±ï¸ Wait a few minutes for all containers to fully initialize.

Check that the containers are running:
```bash
docker ps
```

**8ï¸âƒ£ Import Airflow Connections and Variables**

Once the containers are up, import the pre-configured Airflow settings (connections, variables, etc.) from the file ```airflow_settings.yaml```:
```bash
astro dev object import
```
âœ… This automatically configures your Airflow environment with:
  - Snowflake connection
  - MySQL connection
  - Variables like secret_file, sql_file, and others

**ğŸ”Ÿ Access the Airflow Web UI**

Once Airflow is up and running, open:
```bash
http://localhost:8080
```

Default credentials (Astro):
```bash
Username: admin
Password: admin
```

After logging in, locate the DAG named daily_sales and unpause it to start the workflow.

**9ï¸âƒ£ (Optional) Load Sample Data into MySQL**

To populate the MySQL database with sample data for testing:
```bash
docker exec -i <mysql_container_name> mysql -uroot -proot retail_supply_chain < airflow/data_dummy/store_a.sql
```

âœ… Done!

Your pipeline will now automatically:

  - Create schemas and tables in Snowflake
  - Extract data from MySQL
  - Load raw data into the landing schema
  - Run dbt transformations and snapshots
  - Perform dbt model testing

âš¡ Quick Summary (Cheat Sheet)
```bash 
# 1. Install Docker & Astro CLI
# 2. Pull required images
docker pull mysql:8.0.42-debian
docker pull ghcr.io/dbt-labs/dbt-snowflake

# 3. Configure credentials
# - dbt/my_snowflake_db/profiles.yml
# - airflow/airflow_settings.yml

# 4. Start the environment
astro dev start

# 5. Open Airflow UI
http://localhost:8080
```
